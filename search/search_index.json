{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Over the years I've worked in IT, I've written a lot of documentation.  This site has some pieces I've decided to publish to the internet.</p>"},{"location":"how-this-site-was-built/","title":"How This Site was Built","text":"<p>This site uses Material for MkDocs and Github Pages.</p> <p>Material for MkDocs is a documentation framework that sits on to of MkDocs.  It allows documentation to be written in Markdown, which is then presented in a professional way.</p> <p>Github Pages allows the hosting of static content sites from a GitHub on Github.  This is a free service.</p> <p>By using these two products, it's possible to implement \"documentation-as-code\" in a relatively easy to use fashion that presents well-formatted documentation.</p>","tags":["MkDocs","Material for MkDocs","HOWTO"]},{"location":"how-this-site-was-built/#setup","title":"Setup","text":"","tags":["MkDocs","Material for MkDocs","HOWTO"]},{"location":"how-this-site-was-built/#github-repository","title":"Github Repository","text":"<ol> <li> <p>Go to Github and create a new repository</p> </li> <li> <p>Clone the repository to your local system</p> </li> </ol>","tags":["MkDocs","Material for MkDocs","HOWTO"]},{"location":"how-this-site-was-built/#installing-material-for-mkdocs","title":"Installing Material for MkDocs","text":"<ol> <li>Install Python</li> <li>Install pip</li> <li>Install Material for MkDocs using pip: <code>pip install mkdocs-material</code></li> <li>In the directory containing the cloned repository, run the command to create a new MkDocs project: <code>mkdocs new .</code></li> <li>Update the mkdocs.yml file, adding the site name, URL and the theme settings <pre><code>site_name: Jesse's Doco\nsite_url: https://doco.jpboyce.org\ntheme:\n  name: material\n</code></pre></li> <li>At this point, it's possible to run the development server and view the site locally: <code>mkdocs serve</code></li> </ol>","tags":["MkDocs","Material for MkDocs","HOWTO"]},{"location":"how-this-site-was-built/#publishing-with-github-actions","title":"Publishing with GitHub Actions","text":"<ol> <li>In the repository, create a workflow file.  This workflow will run on any commits pushed to the main or master branch, running the command to deploy the site.  The site contents will be published to a separate branch called <code>gh-pages</code> .github/workflows/ci.yml<pre><code>name: ci\non:\n  push:\n    branches:\n      - master\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v5\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material\n      - run: mkdocs gh-deploy --force\n</code></pre></li> <li>Commit the workflow file and any others to the repository</li> <li>Navigate to the Actions tab on the repository to confirm the workflow is running and completes successfully</li> <li>After the workflow completes, the 2nd branch <code>gh-pages</code> should be visible</li> <li>Navigate to the Repository settings</li> <li>Click on Pages</li> <li>Set the branch to <code>gh-pages</code> and click Save</li> <li>Enter the custom domain for the site and click Save</li> <li>Go to the DNS provider for the custom domain and add a CNAME record for the custom domain that points to <code>&lt;user&gt;.github.io</code></li> <li>Go back to the Pages section for the repository.  It should regularly perform a DNS check.  Once that passes, it will start provisioning a TLS certificate.</li> </ol> <p>Info</p> <p>The certificate provisioning process can take up to 15 minutes</p> <ol> <li>Once the certificate has been provisioned, click the Enforce HTTP checkbox so the site is accessable in a secure fashion    </li> </ol>","tags":["MkDocs","Material for MkDocs","HOWTO"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:active-directory","title":"Active Directory","text":"<ul> <li>            Active Directory - 10 years later          </li> <li>            LAPS          </li> <li>            Security Groups          </li> </ul>"},{"location":"tags/#tag:azure","title":"Azure","text":"<ul> <li>            Bicep Patterns and Practices          </li> </ul>"},{"location":"tags/#tag:bicep","title":"Bicep","text":"<ul> <li>            Bicep Patterns and Practices          </li> </ul>"},{"location":"tags/#tag:centos","title":"CentOS","text":"<ul> <li>            Join Active Directory          </li> </ul>"},{"location":"tags/#tag:chef","title":"Chef","text":"<ul> <li>            Chef Server Install          </li> <li>            Creating Platforms for Test Kitchen          </li> <li>            vRO Integration          </li> </ul>"},{"location":"tags/#tag:citrix","title":"Citrix","text":"<ul> <li>            FAS Setup          </li> </ul>"},{"location":"tags/#tag:cloudflare","title":"Cloudflare","text":"<ul> <li>            Guacamole and Cloudflare          </li> </ul>"},{"location":"tags/#tag:deprecated","title":"Deprecated","text":"<ul> <li>            Chef Server Install          </li> <li>            Creating Platforms for Test Kitchen          </li> <li>            FAS Setup          </li> <li>            Virtual Appliance Install          </li> <li>            vRO Integration          </li> </ul>"},{"location":"tags/#tag:f5","title":"F5","text":"<ul> <li>            Virtual Appliance Install          </li> </ul>"},{"location":"tags/#tag:guacamole","title":"Guacamole","text":"<ul> <li>            Guacamole and Cloudflare          </li> </ul>"},{"location":"tags/#tag:howto","title":"HOWTO","text":"<ul> <li>            How This Site Was Built          </li> </ul>"},{"location":"tags/#tag:laps","title":"LAPS","text":"<ul> <li>            LAPS          </li> </ul>"},{"location":"tags/#tag:material-for-mkdocs","title":"Material for MkDocs","text":"<ul> <li>            How This Site Was Built          </li> </ul>"},{"location":"tags/#tag:mkdocs","title":"MkDocs","text":"<ul> <li>            How This Site Was Built          </li> </ul>"},{"location":"tags/#tag:nutanix","title":"Nutanix","text":"<ul> <li>            Building a QEMU Template Constructor          </li> </ul>"},{"location":"tags/#tag:powershell","title":"Powershell","text":"<ul> <li>            Powershell Best Practices          </li> </ul>"},{"location":"tags/#tag:vro","title":"vRO","text":"<ul> <li>            vRO Integration          </li> </ul>"},{"location":"apache/guacamole-and-cloudflare/","title":"Guacamole and Cloudflare","text":"<p>Guacamole is a remote access solution.  It uses a HTML 5 web front end to allow access to systems using remote access protocols like RDP and VNC.  A common use case for it is providing access to systems behind a firewall.</p> <p>In some situations, this access may be complicated by factors such as not being able to or being unwilling to open ports.  Cloudflare has a Tunnel service where access can be given to systems in this situation without the need for inbound ports.  The architecture of this arrangement is shown below:</p>","tags":["Guacamole","Cloudflare"]},{"location":"apache/guacamole-and-cloudflare/#cloudflare-domain-setup","title":"Cloudflare Domain Setup","text":"<p>Name Server Configuration Changes</p> <p>Updating the name servers will usually take a while to become active.  This will potentially cause a blocker on progressing further steps.  It is suggested to do these steps well in advance</p> <ol> <li>Go to Cloudflare website and create an account</li> <li> <p>Login to the dashboard, click on the Add button in the top right and select Existing domain</p> <p></p> </li> <li> <p>Enter the domain name and click Continue</p> <p></p> </li> <li> <p>Select the Free plan</p> </li> <li>Click the Continue to Activation button</li> <li>Follow the instructions to update the name server records</li> <li>Click the Continue button</li> </ol>","tags":["Guacamole","Cloudflare"]},{"location":"apache/guacamole-and-cloudflare/#guacamole-server-install","title":"Guacamole Server - Install","text":"<ol> <li>Update packages using     <pre><code>sudo apt update\n</code></pre></li> <li>Install the prerequisite packages     <pre><code>sudo apt install build-essential libcairo2-dev libjpeg-turbo8-dev \\\n libpng-dev libtool-bin libossp-uuid-dev libvncserver-dev \\\n freerdp2-dev libssh2-1-dev libtelnet-dev libwebsockets-dev \\\n libpulse-dev libvorbis-dev libwebp-dev libssl-dev \\\n libpango1.0-dev libswscale-dev libavcodec-dev libavutil-dev \\\n libavformat-dev\n</code></pre></li> <li>Get the latest release URL from the releases page and use wget to download it to the server:     <pre><code>sudo wget https://downloads.apache.org/guacamole/1.5.5/source/guacamole-server-1.5.5.tar.gz\n</code></pre></li> <li>Unzip the download     <pre><code>sudo tar -xvf guacamole-server-1.5.5.tar.gz\n</code></pre></li> <li>Change to the extracted folder     <pre><code>cd guacamole-server-1.5.5\n</code></pre></li> <li>Run the build and install commands     <pre><code>sudo ./configure --with-init-dir=/etc/init.d --enable-allow-freerdp-snapshots\nsudo make\nsudo make install\n</code></pre></li> <li>Run the command to update the installed library cache     <pre><code>sudo ldconfig\n</code></pre></li> </ol>","tags":["Guacamole","Cloudflare"]},{"location":"apache/guacamole-and-cloudflare/#guacamole-server-service-config","title":"Guacamole Server - Service Config","text":"<ol> <li>Reload systemd     <pre><code>sudo systemctl daemon-reload\n</code></pre></li> <li>Enable and start the service     <pre><code>sudo systemctl enable guacd &lt;br&gt;sudo systemctl start guacd\n</code></pre></li> <li>Verify it\u2019s running     <pre><code>sudo systemctl status guacd\n</code></pre></li> </ol>","tags":["Guacamole","Cloudflare"]},{"location":"apache/guacamole-and-cloudflare/#guacamole-frontend-install","title":"Guacamole Frontend - Install","text":"<ol> <li>Add the Ubuntu 22.04 repo     <pre><code>sudo add-apt-repository -y -s \"deb Index of /ubuntu  jammy main universe\"\n</code></pre></li> <li>Install Tomcat 9 and support packages     <pre><code>sudo apt install tomcat9 tomcat9-admin tomcat9-common tomcat9-user -y\n</code></pre></li> <li>Download the frontend package     <pre><code>sudo wget https://downloads.apache.org/guacamole/1.5.5/binary/guacamole-1.5.5.war\n</code></pre></li> <li>Move the downloaded war file     <pre><code>sudo mv guacamole-1.5.5.war /var/lib/tomcat9/webapps/guacamole.war\n</code></pre></li> <li>Restart the Tomcat and Guacmole services     <pre><code>sudo systemctl restart tomcat9 guacd\n</code></pre></li> <li>Install database for database auth     <pre><code>sudo apt install mariadb-server -y\n</code></pre></li> <li>Run the mysql security script to set a password for the database     <pre><code>sudo mysql_secure_installation\n</code></pre></li> <li>Download the mySQL Java connector     <pre><code>sudo wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-j-9.1.0.tar.gz\n</code></pre></li> <li>Unzip the download and move to the guac lib folder     <pre><code>sudo tar -xf mysql-connector-j-9.1.0.tar.gz&lt;br&gt;sudo cp mysql-connector-j-9.1.0/mysql-connector-j-9.1.0.jar /etc/guacamole/lib/\n</code></pre></li> <li>Download the JDBC Auth plugin     <pre><code>sudo wget https://downloads.apache.org/guacamole/1.5.5/binary/guacamole-auth-jdbc-1.5.5.tar.gz\n</code></pre></li> <li>Unzip and move files     <pre><code>sudo tar -xf guacamole-auth-jdbc-1.5.5.tar.gz\nsudo mv guacamole-auth-jdbc-1.5.5/mysql/guacamole-auth-jdbc-mysql-1.5.5.jar /etc/guacamole/extensions/\n</code></pre></li> <li>Login to the MariaDB shell     <pre><code>sudo mysql -u root -p\n</code></pre></li> <li>Create the user database and user     <pre><code>CREATE DATABASE guac_db;\nCREATE USER 'guac_user'@'localhost' IDENTIFIED BY 'password';\nGRANT SELECT,INSERT,UPDATE,DELETE ON guac_db.* TO 'guac_user'@'localhost';\nFLUSH PRIVILEGES;\n</code></pre></li> <li>Exit the MariaDB shell     <pre><code>EXIT;\n</code></pre></li> <li>Change to the mySQL schema directory     <pre><code>cd guacamole-auth-jdbc-1.5.5/mysql/schema\n</code></pre></li> <li>Import the schema files     <pre><code>cat *.sql | mysql -u root -p guac_db\n</code></pre></li> <li>Edit the guac properties file     <pre><code>sudo vi /etc/guacamole/guacamole.properties\n</code></pre></li> <li>Add connection details for the database     <pre><code>mysql-hostname: 127.0.0.1\nmysql-port: 3306\nmysql-database: guac_db\nmysql-username: guac_user\nmysql-password: password\n</code></pre></li> <li>Restart services     <pre><code>sudo systemctl restart tomcat9 guacd mysql\n</code></pre></li> </ol>","tags":["Guacamole","Cloudflare"]},{"location":"apache/guacamole-and-cloudflare/#guacamole-frontend-install_1","title":"Guacamole Frontend - Install","text":"<ol> <li>Access the frontend\u2019s URL (ie. http://192.168.1.x:8080/guacamole/), you should see a login page</li> <li>Login with the default credentials of guacadmin</li> <li>Change the default password</li> <li>Add required connections</li> </ol>","tags":["Guacamole","Cloudflare"]},{"location":"azure/patterns-and-practices/","title":"Patterns and Practices","text":"<p>This is an aggregation of information from Microsoft and other sources, as well as my own observations/thoughts.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#general","title":"General","text":"","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#be-mindful-of-biceps-intent-and-capabilities","title":"Be mindful of Bicep's intent and capabilities","text":"<p>Bicep's core capability is to declare Azure infrastructure.  It's syntax is simple.  By it's nature and capabilities, it shouldn't be using fancy tricks or complicated logic.  Attempting to do so also adds cognitive effort to interpret the resulting infrastructure such a Bicep file might create.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#resource-vs-module-vs-verified-module-behaviors","title":"Resource vs Module vs Verified Module Behaviors","text":"<p>One of the nice features of Bicep is being able to reference properties of one resource in another reason.  This allows us to easily and securely pass values between the resources where a relationship may exist.  A very common example of this is Function Apps, which rely on a Storage Account to store state and other data.  When the resources are coded as plain Bicep resources, this is easy to achieve:</p> <pre><code>@description('Storage Account Resource')\nresource storageAccount 'Microsoft.Storage/storageAccounts@2024-01-01' = {\n  name: storageAccountName\n  location: location\n  sku: {\n    name: storageAccountSku\n  }\n  kind: storageAccountKind\n  properties: {\n    supportsHttpsTrafficOnly: true\n    minimumTlsVersion: 'TLS1_2'\n  }\n}\n\n@description('Function App Resource')\nresource functionApp 'Microsoft.Web/sites@2024-04-01' = {\n  name: functionAppName\n  location: location\n  kind: 'functionapp'\n  properties: {\n    siteConfig: {\n      appSettings: [\n        {\n          name: 'AzureWebJobsStorage'\n          value: 'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};AccountKey=${storageAccount.listKeys().keys[0].value};EndpointSuffix=core.windows.net'\n        }\n        {\n          name: 'FUNCTIONS_WORKER_RUNTIME'\n          value: 'dotnet'\n        }\n      ]\n    }\n    serverFarmId: functionAppServicePlan.id\n    httpsOnly: true\n    clientAffinityEnabled: false\n    clientCertEnabled: false\n  }\n}\n</code></pre> <p>As shown in the example, the Function App has a setting called <code>AzureWebJobsStorage</code> that references both the Storage Account's name property and it's access key.</p> <p>If a module is used for the Storage Account, the listKeys Resource function is no longer available.  It is possible to define outputs in the module for the Function App to reference, such as the name using the format `.outputs..  When attempting to use Outputs for secrets (in this case, the access keys), the linter will complain, as shown below: <p></p> <p>This is an issue because inputs and outputs are stored in the details of Deployments, meaning a user with read permission could potentially access the values.  In this situation there is a workaround, having the module output the resourceId and then use that value to safely lookup the keys.</p> <p>With Azure Verified Modules (AVM), the approach is more structured.  AVMs have an approach where secrets are saved to a defined keyvault.  These values can then be referenced by other resources.  This is an approach that can work for regular modules but adds additional resources to manage.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#convention-over-configuration","title":"Convention over Configuration","text":"<p>Convention over Configuration is a software design concept aimed at reducing the number of decisions or inputs required to do something.  It partners well with Infrastructure provisioning and Infrastructure as Code because we often create rules for how resources should be, such as their name and other characteristics.  Using resource naming as an example, if your organisation has a defined naming standard similar to that mentioned in their documentation, it is possible to construct the names (ie. use a convention) rather than have their hard-coded (ie. they become an item in configuration)</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#resource-naming-considerations","title":"Resource Naming Considerations","text":"<p>This item is less about Bicep and more about general naming conventions.  Many Azure resources provide generous sizing for names, with many allowing up to 64 characters.  Some even exceed this.  However, some resources have very small naming lengths.  Some of these include:</p> <ul> <li>Windows virtual machines and scale sets are limited to 15 characters</li> <li>Several resources types such as Elastic SANs, Storage Accounts and Key Vaults are limited to 24 characters</li> </ul> <p>With this in mind, careful consideration needs to be given to the components that make up the naming convention you use.  An extra complication in this issue is Microsoft's own recommended naming convention includes the full region name.  For Australian regions, this results in a very long value.  There is also not a standard set of abbreviations.  Jed Laundry's list of different abbreviations.  Jed recommends using the Terraform CAF short names, which results in most regions being 2 or 3 characters.</p> <p>Similarly for the environment value, it may help to adopt a shortened form of the environment name, such as <code>TST</code> instead of <code>TEST</code> to save on characters.  For the workload/application/project part of the resource name, adopting an abbreviated form can also help.  More verbose information can be reflected in tagging applied to the resources.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#authoring","title":"Authoring","text":"","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#use-a-consistent-entry-point-file-name","title":"Use a consistent entry point file name","text":"<p>Depending on the complexity of the system you are writing Bicep for and how you lay out the repository, there may be many folders and/or services defined in code.  To make it clearer where the start point is for others, use a consistent entry point file.  Based on what is in Microsoft's numerous Bicep repositories, Microsoft has landed on using <code>main.bicep</code> as that starting point.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#bicep-file-structure","title":"Bicep File Structure","text":"<p>Bicep files have a number of Elements that it can contain.  The ordering of these should follow a consistent pattern so it's easy to find different pieces.  These Elements are:</p> <ul> <li>Parameters (<code>param</code>) - values supplied for use in deployment, either via parameter files or command-line parameter values</li> <li>Variables (<code>var</code>) - can be combined with Parameters, other Variables and Functions to create values.  A common use case is resource names</li> <li>Resources (<code>resource</code>) and/or Modules (<code>module</code>) - The actual resource definitions or modules</li> <li>Outputs (<code>output</code>) - Values resulting from deployed resources</li> <li>Functions (<code>func</code>) - Custom functions that you have created (as opposed to built-in functions)</li> <li>Types (<code>type</code>) - Custom types that you have created</li> </ul> <p>The consistent pattern that Microsoft seems to use mimics the ordering of the items in the list above.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#parameters","title":"Parameters","text":"<p>As Microsoft's Bicep documentation notes:</p> <p>\"parameters are for values that need to vary for different deployments\"</p> <p>Common use cases for \"different deployments\" are deploying to different environments (ie. DEV, TEST, PROD) to different regions.  On that basis, the parameters should give coverage on those values.  Going all the way back to ARM templates, the recommendation at the time was to keep parameters to a minimum as well.</p> <p>Parameters have a number of decorators that can apply constraints or metadata to the parameter.  For example, the maximum length can be set as a constraint.  This can be useful in creating an early failure gate.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#testing","title":"Testing","text":"","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#use-a-testing-pipeline","title":"Use a Testing Pipeline","text":"<p>Applying software development practices to IaC, we would want to have a testing pipeline that acts as a quality gate between authoring Bicep code and deploying it.  The Testing Pyramid concept can be applied here, where fast tests that catch the most issues are run first (leading to a \"fail fast\" outcome) with slower, more complicated tests (such as end-to-end testing) being run later in the process.  Applying this concept to Bicep, I've used test pipelines similar in design to the diagram below:</p> <pre><code>graph LR\n    A[Linting] --&gt; B[Quality Check] --&gt; C[What-if Check] --&gt; D[Test Deploy]</code></pre>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#linting-test","title":"Linting Test","text":"<p>Bicep has a built-in linter that can check for syntax errors.  It also checks for \"best practice violations\".  It is part of the functionality in VS Code if the Bicep extension is installed.  It can also be invoked via the Bicep CLI</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#quality-check","title":"Quality Check","text":"<p>Ideally, the infrastructure deployed from Bicep code should align with the pillars of the Well-Architected Framework (WAF), meaning it should be secure, resilient and scalable.  The PSRule tool can access Bicep and ARM templates against rulesets.  It has an included ruleset for assessing against the WAF.  Below is sample output of the tests run against a Storage Account</p> <pre><code>-&gt; func5c1d59a24b601storage : Microsoft.Storage/storageAccounts [9/9]\n\n    [PASS] Azure.Resource.UseTags (AZR-000166)\n    [PASS] Azure.Resource.AllowedRegions (AZR-000167)\n    [PASS] Azure.Storage.BlobAccessType (AZR-000199)\n    [PASS] Azure.Storage.Name (AZR-000201)\n    [PASS] Azure.Storage.Defender.MalwareScan (AZR-000384)\n    [PASS] Azure.Storage.Firewall (AZR-000202)\n    [PASS] Azure.Storage.MinTLS (AZR-000200)\n    [PASS] Azure.Storage.SecureTransfer (AZR-000196)\n    [PASS] Azure.Storage.BlobPublicAccess (AZR-000198)\n</code></pre> <p>PSRule results can also be exported to various formats.  Some of these formats can then present in the Tests tab in Azure DevOps Pipeline executions.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#what-if-check","title":"What-if Check","text":"<p>Bicep has a a <code>what-if</code> command that provide a preview of the changes that will occur if the Bicep file is deployed.  This is similar to Terraform's <code>plan</code> command.  While it gets a bit closer to determining whether a Bicep file will deploy correctly, it's not totally accurate.  There's a gap between what the <code>what-if</code> command will report and what actually happens during deployment.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#test-deployment","title":"Test Deployment","text":"<p>Even with all the previous steps, there are times when a real deployment of Bicep code can fail.  By doing a test deployment, you can get a heads up on the potential issues.  Ideally, the test deployment should be ephemeral, existing only for moments (for a variety of resources, including cost savings).  There may be value in keeping it available in the event of a failed deployment to aid in troubleshooting.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#deploying","title":"Deploying","text":"","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#use-versioned-artifacts-for-deployments","title":"Use Versioned Artifacts for Deployments","text":"<p>Deploying directly from the main branch in software development is generally considered a bad practice.  This is because the main is in a state of flux, changing as content is merged into it.  The version of main branch code that was deployed last week might be radically different from the version that exists today.  This makes rolling back to a known safe state difficult if issues do arise.</p> <p>At the end of the Testing Pipeline, it's easy to implement a task that creates an artifact.  This artifact is versioned and would represent a definitive version of Bicep code that passed tests.  These artifacts can then be used for deployment.  If problems do happen, the last version to successfully deploy can be easily used to roll back.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#use-approval-gates-for-higher-environments","title":"Use Approval Gates for Higher Environments","text":"<p>With Classic Release Pipelines in Azure Devops, it is very easy to add approval gates to a release stage.</p>","tags":["Azure","Bicep"]},{"location":"azure/patterns-and-practices/#use-unique-names-for-deployments","title":"Use Unique Names for Deployments","text":"<p>If a Bicep deployment is executed using the command-line tools, it will use a deployment name that defaults to the same name as the Bicep file.  So a main.bicep file will create deployments called main.  This behaviour will meant that past deployment details will be overwritten by the most recent one.</p> <p>If the <code>ARM Template deployment</code> task in Azure DevOps is used to perform Bicep deployments, it will use a generated name that includes the Bicep file.  This ensures the deployments should have unique names and the deployment details are preserved.</p>","tags":["Azure","Bicep"]},{"location":"centos/join-ad/","title":"Join Active Directory","text":"","tags":["CentOS"]},{"location":"centos/join-ad/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>Ensure network configuration is such that the domain can be looked up (ie. DNS configuration)</li> </ul>","tags":["CentOS"]},{"location":"centos/join-ad/#steps","title":"Steps","text":"<p>Some other sites specify the Python v2 package of policycoreutils.  This doesn\u2019t work on CentOS 7.x or later, so v3 was used</p> <ol> <li>Install required software.     <pre><code>yum install sssd realmd oddjob oddjob-mkhomedir adcli samba-common samba-common-tools krb5-workstation openldap-clients python3-policycoreutils\n</code></pre></li> <li>Join the domain with an account of appropriate rights     <pre><code>realm join --user=administrator contoso.com\n</code></pre></li> <li>Run realm command to verify join     <pre><code>realm list\n\n# Sample output:\n[root@svr31 ~]# realm list\ncontoso.com\n  type: kerberos\n  realm-name: CONTOSO.COM\n  domain-name: contoso.com\n  configured: kerberos-member\n  server-software: active-directory\n  client-software: sssd\n  required-package: oddjob\n  required-package: oddjob-mkhomedir\n  required-package: sssd\n  required-package: adcli\n  required-package: samba-common-tools\n  login-formats: %U@contoso.com\n  login-policy: allow-realm-logins\n</code></pre></li> <li>Verify administrator account     <pre><code>id administrator@contoso.com\n\n# Sample output:\n[root@svr31 ~]#  id administrator@contoso.com\nuid=740800500(administrator@contoso.com)\ngid=740800513(domain users@contoso.com)\ngroups=740800513(domain users@contoso.com),\n  740800520(group policy creator owners@contoso.com),\n  740800512(domain admins@contoso.com),\n  740800518(schema admins@contoso.com),\n  740800572(denied rodc password replication group@contoso.com),\n  740800519(enterprise admins@contoso.com)\n</code></pre></li> <li> <p>Verify Computer object in AD (A computer object should appear in the Computers container as shown below)</p> <p></p> </li> <li> <p>Edit sudo account to include sudo AD group     <pre><code>[root@svr31 ~]# cat /etc/sudoers.d/sudoers\n%sudoers@contoso.com      ALL=(ALL)       ALL\n</code></pre></p> </li> </ol>","tags":["CentOS"]},{"location":"chef/chef-server-install/","title":"Chef Server 12.x Install","text":"<ol> <li>Provision a Linux host</li> <li>Download the installer onto the host     <pre><code>cd /tmp\n# wget https://packages.chef.io/files/stable/chef-server/12.18.14/el/7/chef-server-core-12.18.14-1.el7.x86_64.rpm\nwget https://packages.chef.io/files/stable/chef-server/13.0.17/el/7/chef-server-core-13.0.17-1.el7.x86_64.rpm\n</code></pre></li> <li>Install Chef server     <pre><code># rpm -Uvh /tmp/chef-server-core-12.18.14-1.el7.x86_64.rpm\nrpm -Uvh /tmp/chef-server-core-13.0.17-1.el7.x86_64.rpm\n</code></pre></li> <li>Start all the Chef services     <pre><code>chef-server-ctl reconfigure\n</code></pre></li> <li>Create an administrator account. This command will generate an RSA private key, which is the private key for that user.  Specifying the --filename option allows you to specify the path     <pre><code>chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL 'PASSWORD' --filename FILE_NAME\n</code></pre></li> <li>Create an organisation.  This command will generate an RSA private key, which is the chef-validator key.     <pre><code>chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename ORGANIZATION-validator.pem\n</code></pre></li> <li>Install the management console     <pre><code>chef-server-ctl install chef-manage\nchef-server-ctl reconfigure\n# chef-manage-ctl reconfigure\n# Newer versions need the accept license flag in the command\nchef-manage-ctl reconfigure --accept-license\n</code></pre></li> <li> <p>Logon to the Chef Manage console to verify things are working, using the administrator account details from step 5</p> <p></p> </li> <li> <p>Go to the Policy tab and the Clients menu item.  Verify there is an item for ORGNAME-validator</p> <p></p> </li> <li> <p>Click on the Administration tab and under Organisations, verify there is an item for your organisation</p> <p></p> </li> </ol> <p>To add the Chef server to vRO's inventory, run the Add Chef Host workflow (Library &gt; CHEF &gt; Configuration).  Fill in the form items and submit the workflow</p> <p></p>","tags":["Chef","Deprecated"]},{"location":"chef/platforms-for-test-kitchen/","title":"Creating Platforms for Test Kitchen","text":"<p>A Platform for Test Kitchen is a target environment for Test Kitchen to execute on.  Often a form of Virtual Machine.</p>","tags":["Chef","Deprecated"]},{"location":"chef/platforms-for-test-kitchen/#packer-templates","title":"Packer Templates","text":"<p>A Packer Template is a JSON file that defines the characteristics of a Packer output.  It has some main sections:</p> <ul> <li>builders - Defines the so-called builders that are used to create the machine images.  These are things like VirtualBox, VMware, etc.</li> <li>provisioners - This section defines software that will be installed and configured on the machine</li> <li>post-processors - These define steps after the machine is built</li> <li>variables - Can be used to customise settings</li> <li>communicators - This specifies the interface that Packer uses to get assets on the machine and execute them.  Options are winrm or ssh</li> </ul> <p>Key parts of the vbox-2016.json file that is used:</p> Section Item Notes builders vboxmanage This sets things like RAM and CPU.  I've added some extra customisations that turn on virtualisation options and set the clipboard to bidirectional builders hard_drive_interface This normally defaults to ide.  I've changed this to sata.  Originally tried scsi (which would've made the controller an LSI Logic but installs errored out) builders disk_size Added this to verify that customising C: size can be done (internal default from packer is 40GB) builders hard_drive_nonrotational Setting this to true tells the OS that the HD is a SSD and stops it doing certain tasks like defragging post-processors output This creates a box file ready for immediate use by Vagrant with a timestamped name. post-processors vagrant_template The vagrant template file used when the box file is created.  The only items of note appears to be an override setting that resets memory back to 1GB <p>As a result of the json file used, packer creates a box file that can be picked up by Vagrant.</p>","tags":["Chef","Deprecated"]},{"location":"chef/platforms-for-test-kitchen/#vagrant","title":"Vagrant","text":"<p>Vagrant is used to create \"boxes\".  These boxes are based on the output from Packer and are used by Test Kitchen to spin up virtual machines for testing (Test Kitchen can use a number of providers to provision test machines and has built in support for Vagrant).  To add a box file, use the command:</p> <pre><code>vagrant box add &lt;boxname&gt; &lt;box file path&gt;\n</code></pre> <p>Where <code>&lt;boxname&gt;</code> is the name the box will have once created and <code>&lt;box file path&gt;</code> is the location of the file that Packer created.  Once created, you can verify with a list command.  Example output is shown below.</p> <pre><code>vagrant box add windows2016 C:\\gitrepo\\packer-templates\\windows2016-20180609-virtualbox.box\n==&gt; box: Box file was not detected as metadata. Adding it directly...\n==&gt; box: Adding box 'windows2016' (v0) for provider:\n    box: Unpacking necessary files from: file:///C:/gitrepo/packer-templates/windows2016-20180609-virtualbox.box\n    box: Progress: 100% (Rate: 933M/s, Estimated time remaining: --:--:--)\n==&gt; box: Successfully added box 'windows2016' (v0) for 'virtualbox'!\nvagrant box list\nwindows2016 (virtualbox, 0)\n</code></pre>","tags":["Chef","Deprecated"]},{"location":"chef/platforms-for-test-kitchen/#test-kitchen","title":"Test Kitchen","text":"<p>Test Kitchen is used to test cookbooks as part of local development.  The main items of note in relation to this are:</p> <ul> <li>A kitchen.yml file in the cookbook folder that defines settings like platforms, suites, etc</li> <li>The platforms defined in the yml file are operating systems that the cookbook will be tested on.  Multiple platforms can be specified (such as different distributions and versions of Linux)</li> <li>A suite is a grouping of settings to define a test scenario.  These settings can include a runlist (cookbooks/recipes to be executed), tests (for testing phases) and attributes</li> <li>An instance is a combination of a specific platform and specific suite.  The instance is referenced by combining the names of the platform and suite.  For example, if the platform being used is windows2016 and the suite is default, then the * instance would be default-windows2016</li> <li>The driver is what is used to create, manage and destroy the instance.  In this case, it is Vagrant</li> </ul> <p>The typical lifecycle of using Test Kitchen involves:</p> <ol> <li>Kitchen Create to create your instance</li> <li>Kitchen Converge to apply the cookbook settings (sometimes a 2nd converge to ensure your cookbook is idempotent)</li> <li>Kitchen Verify to run your tests to confirm the cookbook is working as expected</li> <li>Kitchen Destroy to destroy the instance when finished</li> </ol> <p>It is possible to use kitchen test which will perform perform this sequence end to end.  Typically you would use this to verify the end to end process of a cookbook (for example, on multiple platforms and suites), while the steps listed above are used during day to day development.</p> <p>Kitchen and Platforms</p> <p>Most Kitchen commands can accept a Platform option.  If you do not specify a Platform, then the command will execute on all possible instances as defined in your kitchen.yml file (that is, # of platforms x # of suites).</p> <p>You can use the Kitchen List command to quickly see what Instances are available based on the configuration of your kitchen.yml file.  For example:</p> <pre><code>PS C:\\gitrepo\\cb_vra_iaas&gt; kitchen list\nInstance              Driver   Provisioner  Verifier  Transport  Last Action    Last Error\ndefault-windows-2016  Vagrant  ChefZero     Inspec    Winrm      &lt;Not Created&gt;  &lt;None&gt;\n</code></pre> <p>On the basis of this, Kitchen commands can be executed referencing this instance name specifically.  The commands also use \"fuzzy matching\" or some form of regular expression.  So specifying just \"default\" would create instances that use the default suite but use all available platforms (for a scenario where you want to run the default suite on all platforms) or alternatively, specify the platform name by itself (which would create instances of that OS type and for all defined suites if you wanted to run all scenarios on that one OS type).</p> <p>Vagrant Box Name and Platform Name</p> <p>Ensure the name used to create the Vagrant Box is the same name used for the Platform in kitchen.yml  If the names don't match, the kitchen commands will fail due to the kitchen-vagrant driver being unable to find the Vagrant box, as shown below: <pre><code>PS C:\\gitrepo\\cb_vra_iaas&gt; kitchen create\n-----&gt; Starting Kitchen (v1.21.2)\n-----&gt; Creating &lt;default-windows-2016&gt;...\n       Bringing machine 'default' up with 'virtualbox' provider...\n       ==&gt; default: Box 'windows-2016' could not be found. Attempting to find and install...\n           default: Box Provider: virtualbox\n           default: Box Version: &gt;= 0\n       ==&gt; default: Box file was not detected as metadata. Adding it directly...\n       ==&gt; default: Adding box 'windows-2016' (v0) for provider: virtualbox\n           default: Downloading: windows-2016\n    default:\n       An error occurred while downloading the remote file. The error\n       message, if any, is reproduced below. Please fix this error and try\n       again.\n\n       Couldn't open file /C:/gitrepo/cb_vra_iaas/.kitchen/kitchen-vagrant/default-windows-2016/windows-2016\n</code></pre> In this example, the platform name was \"<code>windows-2016</code>\" while the Vagrant box was \"<code>windows2016</code>\" (no hyphen).</p>","tags":["Chef","Deprecated"]},{"location":"chef/platforms-for-test-kitchen/#extra-considerations","title":"Extra Considerations","text":"<p>Some items of extra consideration:</p> <ul> <li>While VirtualBox has a number of virtual disk formats, it appears when creating a Packer template, the created VirtualBox system defaults to VMDK.  VMDK disks are generally fixed in size and can't be expanded after the fact</li> <li>This means if you need a larger C: drive, it has to be performed as part of the initial template creation</li> <li>While it may be possible to increase the disk size later in the chain, it may be too troublesome to do so</li> </ul>","tags":["Chef","Deprecated"]},{"location":"chef/vro-integration/","title":"vRO Integration","text":"<ol> <li>Download the Plugin from the Chef Plugin page on the VMware Exchange (https://marketplace.vmware.com/vsx/solutions/chef-plugin-for-vrealize-orchestrator?ref=company)</li> <li>Login to the vRO control center (https:///vco-controlcenter/config) <li>Navigate to \"Manage Plugins\"</li> <li>Under the heading \"Install Plugin\" click the Browse button and select the Plugin download</li> <li>Click on the Upload button</li> <li> <p>If the install is successful, you will see a confirmation similar to below</p> <p></p> </li>","tags":["Chef","Deprecated","vRO"]},{"location":"citrix/fas-setup/","title":"FAS Setup","text":"<p>Deprecated Content</p> <p>This content relates to the Citrix Xenapp Essentials product, which has been discontinued</p> <ol> <li>Run the installer and complete it</li> <li> <p>Load the Administration Console</p> <p></p> </li> <li> <p>Click on the Deploy certificate templates item.  You will see a prompt about the certificates that will be installed.  Click OK.  The status should update with a green tick if successful</p> <p></p> </li> <li> <p>Click on the Publish button for the 2nd item.  A prompt will appear again with info about the cert templates and to select the Certificate Authority to publish to.  Click OK to complete the deployment.</p> <p></p> </li> <li> <p>If you manage the certificate authority, the cert templates should appear as available when this step is completed</p> <p></p> </li> <li> <p>Click on the Authorise button.  A confirmation window will appear, select the CA and click OK.  The step will sit in a pending state (spinning waiting icon).</p> <p></p> </li> <li> <p>Go to your CA MMC and navigate to Pending requests</p> <p></p> </li> <li> <p>There should be a pending request from the FAS server.  Right-click and select All Tasks &gt; Issue</p> <p></p> </li> <li> <p>The Authorise set in the FAS setup should update with a positive result.</p> </li> <li> <p>Click on the Create button for the Create a Rule task.  This will start the Create a Rule wizard.  For now, select the creation of a default rule.</p> <p></p> </li> <li> <p>Once the wizard has been completed, click on the Connect button to connect to Citrix Cloud.  The wizard will start.</p> <p></p> </li> <li> <p>As per the instructions, copy the Confirmation URL into a browser with internet access and the code.</p> <p></p> </li> <li> <p>In the browser, click on Register button</p> <p></p> </li> <li> <p>Enter the confirmation code</p> <p></p> </li> <li> <p>A confirmation screen will appear with details of the registration.  Review and click Register</p> <p></p> </li> <li> <p>Once the registration is completed on the Citrix Cloud side, the FAS wizard should update with the resource location to select.</p> <p></p> </li> <li> <p>In the Choose a rule prompt, select the appropriate option (most likely use an existing rule since we created the default one)</p> <p></p> </li> <li> <p>Review the summary screen and click Finish</p> </li> <li>Setup a GPO that sets the FAS Server for appropriate targets as per Install and configure | Federated Authentication Service (citrix.com)</li> </ol>","tags":["Citrix","Deprecated"]},{"location":"f5/applicance-setup/","title":"Virtual Appliance Install","text":"","tags":["F5","Deprecated"]},{"location":"f5/applicance-setup/#installation-configuration-of-virtual-appliance","title":"Installation &amp; Configuration of Virtual Appliance","text":"<ol> <li>Deploy appliance as normal and boot</li> <li>Login to the console with the default credentials (root/default)</li> <li>Run the config command</li> <li> <p>Set an IP address for the management interface or leave as DHCP</p> <p></p> </li> <li> <p>Access the management interface on the new set IP address (https:///) with the default management credentials (admin/admin) <li>Start the Setup Utility</li> <li> <p>Click the Activate License button and fill in the details in the form</p> <p></p> </li> <li> <p>Accept the License Agreement</p> </li> <li>The configuration process will start.  When it is finished click the Continue button</li> <li>Continue the Setup wizard to completion</li>","tags":["F5","Deprecated"]},{"location":"f5/applicance-setup/#create-a-node","title":"Create a Node","text":"<ol> <li>Navigate to Local Traffic &gt; Virtual Servers &gt; Nodes</li> <li>Click on Create</li> <li> <p>Enter details</p> <p></p> </li> <li> <p>Click on the Finished button</p> </li> </ol>","tags":["F5","Deprecated"]},{"location":"microsoft/ad-10-years-later/","title":"Active Directory - 10 years later","text":"<p>Notes on the changes that Active Directory has experienced since it's release.</p>","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#forestdomain-design-in-and-out","title":"Forest/Domain Design - In and Out","text":"Out In Multi-domain forests Domain trusts Single-domain forests Federated identity &amp; claims-based authentication with ADFS","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#general-notes","title":"General Notes","text":"","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#ou-design","title":"OU Design","text":"<ul> <li>Design for security first (delegation, administration, ACLs)</li> </ul>","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#group-policy","title":"Group Policy","text":"<ul> <li>Scoping<ul> <li>Use WMI filters as little as possible</li> <li>Global security group filters are good</li> <li>Always make a deny group</li> </ul> </li> <li>Testing<ul> <li>Can be done in production if filtered correctly</li> </ul> </li> </ul>","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#replication","title":"Replication","text":"<ul> <li>What's changed<ul> <li>Networks are good (more throughput, more reliable)</li> <li>Increased need for convergance</li> <li>People trust AD</li> </ul> </li> <li>Notification based replication<ul> <li>Change inter-site to use notification-based</li> <li>Reduces convergence</li> <li>Reduces issues related to password cahnges, group changes, account lockouts, etc.</li> </ul> </li> </ul>","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#accounts","title":"Accounts","text":"<ul> <li>Regular users in a general users OU</li> <li>Administrative users<ul> <li>In another OU</li> <li>Define types<ul> <li>Client support \u2013 local admin on PCs</li> <li>Server support \u2013 local admin on servers</li> <li>Service admins- rights in Exchange, SQL,etc</li> <li>AD data admin \u2013 can modify objects in AD</li> <li>AD service admin \u2013 physical access to DCs, restore AD</li> </ul> </li> </ul> </li> <li>Groups<ul> <li>DOmain admins, enterprise admins, administrators<ul> <li>As empty as possible</li> </ul> </li> <li>Schema admins<ul> <li>Empty.  Add members when schema updates are to be performed</li> </ul> </li> <li>Built in (account/server/print/backup operators)<ul> <li>Too much access?</li> </ul> </li> <li>Turn on auditing to track cahnges to these groups</li> </ul> </li> </ul>","tags":["Active Directory"]},{"location":"microsoft/ad-10-years-later/#custom-mmcs","title":"Custom MMCs","text":"<ul> <li>Makes transition to separate admin accounts easier</li> <li>Interesting implementations<ul> <li>RDP (not in the default MMC snapin list?)</li> <li>Link to web address (can be used to access a UNC path using admin credentails, gets around explorer issues)</li> <li>Saved queries (locked accounts, disabled,etc.)</li> <li>Taskpad views (allows custom tasks/commands)</li> </ul> </li> </ul>","tags":["Active Directory"]},{"location":"microsoft/laps/","title":"LAPS","text":"","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#summary","title":"Summary","text":"<p>The \"Local Administrator Password Solution\" (LAPS) provides management of local account passwords of domain joined computers. Passwords are stored in Active Directory (AD) and protected by ACL, so only eligible users can read it or request its reset</p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#requirements","title":"Requirements","text":"<p>The following requirements are needed to run LAPS:</p> Requirement Item Value Active Directory Windows 2003 SP1 or above Managed machines Windows 2003 with current SP, or above  Windows Vista with current SP, or above Management Tools .NET Framework 4.0  PowerShell 2.0 or above","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#installation-setup","title":"Installation &amp; Setup","text":"","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#installer-experience","title":"Installer Experience","text":"<p>When running the installer, there are two sets of components that can be installed: The AdmPwd GPO extension and the management tools.  The GPO extension is the part that is required on systems to be managed by LAPS.  The management tools are used to manage LAPS.  To perform the initial setup of LAPS, the management tools will be required.</p> <p></p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#ad-schema-extension","title":"AD Schema Extension","text":"<p>To support LAPS, the AD Schema needs to be extended.  This can be achieved by using the PowerShell module associated with LAPS called AdmPwd.PS.  The user performing these commands must be a Schema Admin.</p> <ol> <li>Import the module    <pre><code>Import-Module AdmPwd.PS\n</code></pre></li> <li>Run the command to update the schema    <pre><code>Update-AdmPwdADSchema\n</code></pre></li> </ol>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#permission-delegation","title":"Permission Delegation","text":"<p>Delegation of permissions needs to be performed on the Organisational Units containing the computer accounts that will be placed under LAPS.  There are several key areas relating to the permissions:</p> <ul> <li>Removing <code>All Extended Permissions</code> from users and groups that won\u2019t be allowed to read the value of the password</li> <li>Adding <code>Write</code> permission to the two LAPS attributes on computer objects for the <code>SELF</code> built-in account.  This is so a machine can update its own password details</li> <li>Adding <code>CONTROL_ACCESS</code> on the <code>ms-Mcs-AdmPwd</code> attribute of computer objects for the group/user that will be allowed to read the password</li> <li>Add <code>Write</code> permission on <code>ms-Mcs-AdmPwdExpirationTime</code> attribute for the group/user that will be allowed to force a password reset</li> </ul> <p>These can be achieved using the following PowerShell commands</p> <p>Step</p> <p>Script/Screenshot</p> <ol> <li>Import the module    <pre><code>Import-Module AdmPwd.PS\n</code></pre></li> <li>Remove <code>All Extended Permissions</code> permission.  The command should affect sub-containers of the OU specified.    <pre><code>Find-AdmPwdExtendedRights -Identity &lt;OU NAME&gt;\n</code></pre></li> <li>Add <code>Write</code> permission for the attributes to <code>SELF</code>. The command should affect sub-containers of the OU specified    <pre><code>Set-AdmPwdComputerSelfPermission -Identity &lt;OU NAME&gt;\n</code></pre></li> <li>Add <code>CONTROL_ACCESS</code> on the Password attribute. The command should affect sub-containers of the OU specified    <pre><code>Set-AdmPwdReadPasswordPermission -Identity &lt;OU NAME&gt; -AllowedPrincipals &lt;allowed user/groups&gt;\n</code></pre></li> <li>Add <code>Write</code> permission on the expiration attribute.    <pre><code>Set-AdmPwdResetPasswordPermission -Identity &lt;OU NAME&gt; -AllowedPrincipals &lt;allowed user/groups&gt;\n</code></pre></li> </ol>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#installation-of-cse","title":"Installation of CSE","text":"<p>The LAPS installer needs to be run on the target machines.  It can be installed silently.  </p> <p>Step</p> <p>Script/Screenshot</p> <ol> <li>Install LAPS GPO CSE    <pre><code>msiexec /q /i &lt;path&gt;\\LAPS.&lt;platform&gt;.msi\n</code></pre></li> <li>Confirm installation</li> </ol> <p></p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#enable-auditing-of-password-resets","title":"Enable Auditing of Password Resets","text":"<p>It\u2019s possible to enable auditing of password resets.  This can be achieved using the following PowerShell command.  Auditing of Directory Service Access events also needs to be enabled on Domain Controllers for audit events to be logged successfully.</p> <pre><code>Set-AdmPwdAuditing -Identity:&lt;OU NAME&gt; -AuditedPrinciples:&lt;security principals to audit&gt;\n</code></pre>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#group-policy","title":"Group Policy","text":"<p>The specific settings related to LAPS are controlled by Group Policy.</p> <ol> <li>If using a Central Store, copy the <code>AdmPwd.admx</code> and <code>AdmPwl.adml</code> from <code>C:\\Windows\\PolicyDefinitions</code> to the Central Store location (ie <code>\\\\contoso.com\\SYSVOL\\contoso.com\\Policies\\PolicyDefintions</code>)</li> <li> <p>Create a new Group Policy Object and open it for editing.  Expand the Administrative Templates branch.  You should see a LAPS entry</p> <p></p> </li> <li> <p>Open the <code>Enable local admin password management</code> item and set it to <code>Enabled</code>.  Click OK to save</p> <p></p> </li> <li> <p>Open the <code>Password Settings</code> item.  Set it to <code>Enabled</code> and then set the password options as required.  Click OK to save.</p> <p></p> </li> <li> <p>If you want to manage a custom local administrator account, open the <code>Name of administrator account to manage</code>.  Set to Enable and enter the name of the custom admin account</p> <p></p> </li> <li> <p>Once all the settings are done, close the GPO editor window</p> </li> <li>Link the GPO to the Organisation Unit containing the systems to be managed by LAPS</li> </ol>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#operations","title":"Operations","text":"","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#how-to-get-the-passwords","title":"How to get the passwords","text":"<p>Passwords can be viewed several ways: via PowerShell, in the AD Users and Computers MMC and using the LAPS UI tool.</p> <ol> <li> <p>PowerShell.  Use the <code>Get-AdmPwdPassword</code> cmdlet and specify the ComputerName    <pre><code>Get-AdmPwdPassword -ComputerName &lt;computerName&gt;\n</code></pre> </p> </li> <li> <p>AD Users and Computers MMC.  Open the Computer Object and select the Attribute Editor tab.  Scroll down to the <code>ms-Mcs-AdmPwd</code> attribute</p> <p></p> </li> <li> <p>LAPS UI Tool.  Run the Tool and enter the Computer Name in the field.  Click Search</p> <p></p> </li> </ol>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#how-to-force-a-reset","title":"How to force a reset","text":"<p>In some circumstances, it may be required to reset the password.  This can be performed using PowerShell or the LAPS UI tool.</p> <ol> <li>Powershell     <pre><code>Reset-AdmPwdPassword -ComputerName &lt;computerName&gt;\n</code></pre></li> <li> <p>LAPS UI Tool.  Run the tool and search for the target system.  Set the desired expiration time (which will force a reset at that time) and click the Set button</p> <p></p> </li> </ol>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#experience-of-non-authorised-users","title":"Experience of Non-Authorised Users","text":"<p>If a user isn\u2019t part of the set of allowed principals used for configuration of LAPS, they won\u2019t see the password.  For example, if a non-authorised user tries to use the PowerShell cmdlet to read the password, no value is returned:</p> <p></p> <p>If attempting to view the password in AD Users and Computers, only a blank is shown.  If the same non-authorised user tries to force a password reset, they will be denied.</p> <p></p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#using-audit-data","title":"Using audit data","text":"<p>If auditing is enabled, event log entries will be generated in the Security log of Domain Controllers.  An example is event ID 4662 which is generated when a password is successfully read.  The event will contain details of the user who read the password and related object (ie. the target computer).  An example of a successful read event is below:</p> <p></p> <p>The contents of the red box is the user account that attempted the reading of the password (in this case, the domain admin account).  The green box shows the details of the object that was read (SVR22).</p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#log-data","title":"Log Data","text":"<p>By default, LAPS will log errors only.  The logging occurs in the local Event Log with the source of <code>AdmPwd</code>.  The log level can be increased by editing the registry key <code>HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\GPExtensions{D76B9641-3288-4f75-942D-087DE603E3EA}</code>, value <code>ExtensionDebugLevel</code> (type <code>DWORD</code>).  To log errors and warnings, set to 1.  To enable verbose logging, set to 2.  An example of events logged with verbose logging is below:</p> Event ID Description 15 Beginning Processing 5 Validation passed for new local admin password 13 Local Administrator\u2019s password has been reported to AD 12 Local Administrator\u2019s password has been changed 14 Finished successfully","tags":["LAPS","Active Directory"]},{"location":"microsoft/laps/#references","title":"References","text":"<p>Microsoft LAPS usage assessment - Microsoft Defender for Identity</p> <p>Download Local Administrator Password Solution (LAPS) from Official Microsoft Download Center</p> <p>https://4sysops.com/archives/introduction-to-microsoft-laps-local-administrator-password-solution/</p> <p>A Look at the Microsoft LAPS Group Policy Settings | Int64 Software Blog</p>","tags":["LAPS","Active Directory"]},{"location":"microsoft/powershell-best-practices/","title":"Powershell Best Practices","text":"<p>An aggregation of various recommendations</p> <ul> <li>Start script with a standard set of comments such as name, date, purpose, etc. </li> <li>Use #Requires to set minimum PS version required, modules, etc. </li> <li>Use Set-StrictMode -Version Latest to prevent the use of unitialised variables (similar to Option Explicit in VBScript) </li> <li>Use simple but meaningful variable names.  Camel Case is \"the best practice\" </li> <li>Use code signing </li> <li>Don't use aliases, use full cmdlet anmes and named parameters </li> <li>Don't use backticks </li> <li>Filter left, format right </li> <li>Add .exe for any external commands/apps being called </li> <li>Use structured error handling (with Try, Catch, Finally/Trap) </li> <li>Use cmdletBinding and add support for -Whatif, Verbose, Debug </li> <li>Use Advanced Functions </li> <li>Use Verb-Noun naming for functions, etc.  For the Verbs, use a standard verb (Get-Verb lists these) </li> <li>Use standard parameter naming (ie. ComputerName is good, Machine or Server is bad) and set to a default value if relevant/possible (ie. $ComputerName = $ENV:ComputerName) </li> <li>A function should do  thing </li> <li>A function should return an object or an array of objects, not formatted text </li> <li>Avoid using the Return keyword.  Functions automatically return output to the calling process via the pipeline </li> <li>Use write-output over write-host (write-host only writes to the host, not the pipeline) </li> <li>Use comment-based help </li> <li> <p>Use Test-Connection to check if a target is online </p> </li> <li> <p>Use full cmdlet names </p> </li> <li>Use named parameters </li> <li>Avoid write-host </li> <li>Use CIM cmdlet (not WMI cmdlet) </li> <li>For Verb-Noun, use a singular form </li> <li>Use approved verbs </li> <li>Use Whatif/Confirm parameters </li> <li>Use custom folding regions </li> <li>Use Set-StrictMode </li> <li>Avoid Out-Null </li> <li>Specify the extension for applications (ie. .exe) </li> <li>Execute scripts with -NoProfile Parameter </li> <li>Check parameters with PSBoundParameters (PSBoundParamters is a hashtable of parameters passed to a script of function) </li> <li> <p>Use #Requires </p> </li> <li> <p>Use approved verbs for commands (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/approved-verbs-for-windows-powershell-commands?view=powershell-7.1)</p> </li> <li>Don\u2019t use special characters in cmdlet names (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#cmdlet-names-characters-that-cannot-be-used-rd02)</li> <li>Don\u2019t use common parameter names for your parameters (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/common-parameter-names?view=powershell-7.1)</li> <li>Support \u201cConfirm\u201d reqests.  If the cmdlet modifies the system, it should allow the use of confirmation (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.* 1#support-confirmation-requests-rd04)</li> <li>Support \u201cForce\u201d parameter for interactive sessions (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#support-force-parameter-for-interactive-sessions-rd05)</li> <li>Document the objects that are written to the pipeline/outputted by your cmdlet (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#document-output-objects-rd06)</li> <li>Derive from the Cmdlet or PSCmdlet Classes (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#derive-from-the-cmdlet-or-pscmdlet-classes-rc01)</li> <li>Specify the Cmdlet Attribute (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#specify-the-cmdlet-attribute-rc02)</li> <li>Override an Input Processing Method (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#override-an-input-processing-method-rc03)</li> <li>Specify the OutputType Attribute (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#specify-the-outputtype-attribute-rc04)</li> <li>Do Not Retain Handles to Output Objects (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#do-not-retain-handles-to-output-objects-rc05)</li> <li>Handle Errors Robustly (https://docs.microsoft.com/en-us/powershell/scripting/developer/cmdlet/required-development-guidelines?view=powershell-7.1#handle-errors-robustly-rc06)</li> </ul>","tags":["Powershell"]},{"location":"microsoft/powershell-best-practices/#references","title":"References","text":"<ul> <li>https://blogs.technet.microsoft.com/pstips/2014/06/17/powershell-scripting-best-practices/</li> <li>http://powershell-guru.com/best-practices/</li> </ul>","tags":["Powershell"]},{"location":"microsoft/security-groups/","title":"Security Groups","text":"<p>Notes on the different types of Active Directory security groups</p>","tags":["Active Directory"]},{"location":"microsoft/security-groups/#group-scope","title":"Group Scope","text":"Group Scope Group can include as members... Group can be assigned permissions in... Group scope can be converted to... Universal Accounts from any domain within the forest in which this Universal Group residesGlobal groups from any domain within the forest in which this Universal Group residesUniversal groups from any domain within the forest in which this Universal Group resides Any domain or forest Domain localGlobal (as long as no other universal groups exist as members) Global Accounts from the same domain as the parent global groupGlobal groups from the same domain as the parent global group Member permissions can be assigned in any domain Universal (as long as it is not a member of any other global groups) Domain local Accounts from any domainGlobal groups from any domainUniversal groups from any domainDomain local groups but only from the same domain as the parent domain local group Member permissions can be assigned only within the same domain as the parent domain local group Universal (as long as no other domain local groups exist as members) Group Scope Members from same domain Members from other domain, same forest Members from external domain/forest (via trust) Local <ul><li>Users</li><li>Computers</li><li>Global Groups</li><li>Universal Groups</li><li>Domain Local</li><li>Other local groups</li></ul> <ul><li>Users</li><li>Computers</li><li>Global Groups</li><li>Universal groups</li></ul> <ul><li>Users</li><li>Computers</li><li>Global groups</li></ul> Domain Local <ul><li>Users</li><li>Computers</li><li>Global</li><li>Universal</li><li>Domain Local</li></ul> <ul><li>Users</li><li>Computers</li><li>Global</li><li>Universal</li></ul> <ul><li>Users</li><li>Computers</li><li>Global</li></ul> Universal <ul><li>Users</li><li>Computers</li><li>Global</li><li>Universal</li></ul> <ul><li>Users</li><li>Computers</li><li>Global</li><li>Universal</li></ul> <ul><li>N/A</li></ul> Global <ul><li>Users</li><li>Global groups</li></ul> <ul><li>N/A</li></ul> <li>N/A</li>","tags":["Active Directory"]},{"location":"microsoft/security-groups/#when-to-use-a-type-of-group","title":"When To Use A Type of Group","text":"<ul> <li>Domain Local \u2013 Access/Permission to a resource (ie. Users -&gt; Domain Local Group -&gt; Folder/Printer)</li> <li>Global - Maintenance of objects where change doesn't require replication outside of the domain (ie. Users/computer accounts in that domain) or for setting permissions on domain directory objects that are replicated in the Global Catalog.  Suited for use as Role groups.  Only replicated in the domain they exist in.  Most limited in membership (only same domain), most flexible in availability (available in same domain/forest, trusted forests)</li> <li>Universal \u2013 To consolidate groups that span domains (Users -&gt; Global Group -&gt; Universal Group).  Changes in the Global Group membership won't impact the Universal group.  Useful in multi-domain forests.  Replicated to global catalog.  Can't include members from another forest,</li> </ul>","tags":["Active Directory"]},{"location":"microsoft/security-groups/#best-practice-model","title":"Best Practice Model","text":"<p>Use the <code>IGDLA</code> (Identity, Global Group, Domain Local group, Access) model.  Previously known as <code>AGDLP</code> (Account, Global group, Domain Local Group, Permissions).</p> <ul> <li>Users in \"Roles\" Groups (human readable, align with HR/position info if possible)</li> <li>Define naming convention for other groups, ie. <code>&lt;grouptype&gt;-&lt;resource&gt;-&lt;right&gt;</code>, G-Finance-Read</li> <li>Only Domain Local and Local groups can include objects from another forest</li> <li>Nesting strategy<ul> <li>Basic (<code>IGDLA</code>) - Identity (user account) -&gt; Global Group (role group) -&gt; Domain Local (defines resource &amp; rights) -&gt; ACL on resource</li> <li>Recommended/multi-domain (<code>IGUDLA</code>) - Identity (user account) -&gt; Global Group (role group) -&gt; Universal Group (linking/glue group for other domains) -&gt; Domain Local (defines resource &amp; rights) -&gt; ACL on resource</li> </ul> </li> <li>Group membership is evaluated when the user logs on (security token generation) so any membership changes require a relogon (still true for claims/other methods?)</li> </ul>","tags":["Active Directory"]},{"location":"nutanix/qemu-template/","title":"Building a QEMU Template Constructor","text":"<p>The Nutanix AHV VM format is based on QEMU.  In order to build VMs for the AHV platform, you need to provision a VM with the appropriate settings.</p> <ul> <li>Due to a bug in Red Hat, the two SCSI controller options (scsi and virtio-scsi) can't be used.  Unfortunately, the default controller in Nutanix is the virtio-scsi controller.</li> <li>The stable release of Debian doesn't have a recent enough version of QEMU associated with it, so the unstable release was used</li> </ul>","tags":["Nutanix"]},{"location":"nutanix/qemu-template/#setup","title":"Setup","text":"<ol> <li>Create a Virtual Machine in VirtualBox and ensure that the settings for Nested Virtualisation are enabled     </li> <li>Provision the virtual machine</li> <li>Confirm that virtualisation features are exposed in the VM     <pre><code>lscpu | grep Virtualization\n</code></pre></li> <li>Install the core QEMU tools     <pre><code># Debian\n# Update current packages\napt update\n\n# Install QEMU\napt install qemu qemu-kvm\n</code></pre></li> <li>At this point, you should be able to create VMs</li> <li>Install packer     <pre><code># Install wget\napt install -y wget\n\n# Download packer\nwget https://releases.hashicorp.com/packer/1.3.5/packer_1.3.5_linux_amd64.zip\n\n# Install unzip\napt install -y unzip\n\n# Unzip packer zip file\nunzip packer_1.3.5_linux_amd64.zip\n</code></pre></li> <li>If you intend to build VMs with headerless mode set to false, then you will need to install a GUI.  To the right is the commands to install the GNOME desktop interface     <pre><code># Install GNOME Desktop\napt -y groups install \"GNOME Desktop\"\n</code></pre></li> </ol>","tags":["Nutanix"]},{"location":"nutanix/qemu-template/#building-a-template","title":"Building a Template","text":"<ol> <li>Transfer installation media to the QEMU server</li> <li>Stage files for creating the template via packer on the QEMU server</li> <li>Run packer pointing to the json file     <pre><code>./packer build quemu-centOS-7-base.json\n</code></pre></li> <li> <p>If successful, output similar to this will happen</p> <p></p> </li> </ol>","tags":["Nutanix"]}]}